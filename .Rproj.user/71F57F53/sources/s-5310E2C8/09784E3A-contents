---
title: "ISQA 8700 Assignment 5"
author: "Alex Daly"
date: "4/4/2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(readxl)
library(tree)
library(rpart)
library(rpart.plot)
```

I started off by taking the JobCat data set and cleaning it up. I also split the data into the training and testing data sets.

```{r dataLoad, echo=TRUE}
set.seed(123)
appData <- read_excel("JobCat.xls")
appData <- subset(appData, select = -c(2))
appTrainingData <- appData %>% filter(row(appData) >= 1 & row(appData) <= 250)
appTestingData <- appData %>% filter(row(appData) >= 251 & row(appData) <= 500)
```

I then used the rpart function to create a decision tree for Outcome. I made the tree use classification. The below diagram is what resulted. Only the Major, Degree, and State columns were used to classify the outcomes. Only 3 of the 4 possible outcomes were used. Excellent was omitted.

```{r decisionTree, echo=TRUE}
modelTree <- rpart(Outcome ~ age + state + degree + major + experience, data=appTrainingData, method="class")
prp(modelTree)
```

I used the model to classify the training data set and created a coincidence matrix. It correctly classified 72.8% of the rows.

```{r trainingMatrix, echo=TRUE}
modelTrainingPredictions = predict(modelTree, data=appTrainingData, type="class")
table(appTrainingData$Outcome, modelTrainingPredictions)
```

Applying the model to the testing data set, I made another coincidence matrix. It only correctly classified this data set 42% of the time. This tells me that the model needs more refinement and that we probably need more data to do that.

```{r testingMatrix, echo=TRUE}
modelTestingPredictions = predict(modelTree, data=appTestingData, type="class")
table(appTestingData$Outcome, modelTestingPredictions)
```